---
title: 'Homework 2: text processing'
author: "Your Name"
date: "Today's date"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

The second homework assignment covers text processing and associated skills, including textual statistics and dictionary methods.  

## Analysis of tweets during a political crisis

We will start with a dataset that contains almost 900 tweets that were published by four central figures in American politics around the time of the onset of an impeachment inquiry: Pres. Donald Trump, Rudy Giuliani, Speaker of the House Rep. Nancy Pelosi, and Chair of the House Intelligence Committee Rep. Adam Schiff.  

First, read in the spreadsheet of tweets into R and then use the `str` and `head` functions to describe the variables and contents of the dataset.  Be sure that the file is in the same folder as this homework RMarkdown file.

```{r}
data <- read.csv("us_tweets.csv", 
                 stringsAsFactors=FALSE,
                 encoding = "utf-8")
```

Print the number of tweets that are in this dataset.

```{r}
# Insert code here
```

Create a new dataframe that only includes original tweets (i.e. remove retweets).

```{r}
ndata <- data[which(),] # Fill in the gaps
```

Create a smaller dataframe that only includes tweets by Donald Trump.

```{r}
trump <- ndata[which(), ]# Fill in the gaps
```

How many tweets include an exclamation mark? In how many tweets did Trump mention words related to "winning", "employment", "immigration" or "hoax"? Use regular expressions when searching the tweets; you may also wish to wrap your search term between word anchor boundaries (`\\b`).  For instance, for the term health: `"\\bhealth\\b"`

```{r}
sum(grepl("", trump$text, ignore.case = TRUE)) # Adapt this code as needed
```

## Corpus creation

Create a `corpus` and a `dfm` object with processed text (including collocations) using the dataframe generated in Question 1.1.  

```{r}
library(quanteda)

# create corpus
corpus <- 
  
# create tokens object
toks <- tokens(corpus,
               include_docvars = TRUE) %>%
  tokens_????() %>% # which function transforms to lower case?
  tokens_????(stopwords('english'), padding = TRUE) %>% # which function for these three lines?
  tokens_????('[\\p{P}\\p{S}]', valuetype = 'regex', padding = TRUE) %>%
  tokens_????('amp', valuetype = 'fixed', padding = TRUE)
  
# detect collocations and merge with tokens object (choose your own parameters)
col <- textstat_????(???, # fill in the blanks here
                             method = "", # which method? 
                             size = ?, # which size?
                             min_count = ?, # how many?
                             smoothing = 0.5
)

toks <- tokens_compound(toks, pattern = col[col$z > ?]) # choose a z score
toks <- tokens_remove(tokens(toks), "") # this code removes whitespace

# create dfm from tokens object
docfm <- dfm(toks,
             remove_numbers = TRUE,
             remove_punct = TRUE,
             remove_symbols = TRUE,
             remove_hyphens = TRUE,
             remove_separators = TRUE,
             remove_url = TRUE)

docfm <- dfm_select(docfm, pattern = stopwords("en"), selection = "remove")
```

## Textual statistics

With the generated `dfm` object perform the following tasks:

- Create a frequency plot of the top 30 tokens for Trump and Pelosi.

```{r}
library(ggplot2)

# Trump plot
dfm_trump <- dfm_????(docfm, screen_name == "???") # Fill in the blanks

dfm_freq <- textstat_????(dfm_trump, n = ??) # Fill in the blanks

dfm_freq$feature <- with(dfm_freq, reorder(feature, -frequency))

ggplot(dfm_freq, aes(x = ???, y = ???)) + 
  ggtitle("???") +
  geom_point() + 
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Pelosi plot
# your code  here
```

- Determine the "key" terms that Trump and Pelosi are more likely to tweet.  Plot your results.

```{r}
dfm_comparison <- dfm_subset(docfm, ???) # Use the correct argument for subsetting here

set.seed(1234)

dfm_keyness <- dfm_group(dfm_comparison, groups = "???") # select the correct variable to group on

keyness_stat <- textstat_???(dfm_keyness, target = "realDonaldTrump") # choose the correct function

textplot_???(keyness_stat, labelsize = 3) # choose the correct function
```

```{r}
#Trump keyness
head(keyness_stat, 30)
```

```{r}
# Pelosi keyness
# your code here
```

- Perform a keyword in context analysis using your `corpus` object for some of the most distinct keywords from both Trump and Pelosi. *Hint: remember to use the `phrase` function in the `pattern` argument of `kwic`*

```{r}
# Trump
trump_corp <- corpus_subset(corpus, screen_name %in% "realDonaldTrump")
trump_kwic1 <- kwic(trump_corp, pattern = phrase("witch hunt"), window = 5, case_insensitive = TRUE)
trump_kwic2 <- kwic(trump_corp, pattern = phrase("great"), window = 5, case_insensitive = TRUE)
trump_kwic3 <- kwic(trump_corp, pattern = phrase("ukraine"), window = 5, case_insensitive = TRUE)
head(trump_kwic1)

# Pelosi
# Your code here
```

## Dictionary methods

Conduct a sentiment analysis of Trump's tweets using the Lexicon Sentiment Dictionary.  Plot net sentiment over the entire sample period and interpret the results.

```{r}
sent_dfm <- dfm(???, dictionary = data_dictionary_LSD2015[1:2]) # Fill in the blanks

docvars(dfm_trump, "prop_negative") <- as.numeric(sent_dfm[,?] / ntoken(???)) # Fill in the blanks
docvars(dfm_trump, "prop_positive") <- as.numeric(sent_dfm[,?] / ntoken(???)) # Fill in the blanks

docvars(dfm_trump, "net_sentiment") <- # complete the code

docvars(dfm_trump, "date2") <- # use the lubridate package to parse the date

sent_plot <- ggplot(???, aes(x = ???, y = ???)) + # Fill in the blanks
  geom_smooth() +
  theme_minimal()

sent_plot
```

What can we learn about the political communication surrounding the political crisis based on the above results?
