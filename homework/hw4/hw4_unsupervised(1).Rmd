---
title: 'Homework 4: unsupervised classification'
author: "Your Name"
date: "Today's date"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

The final homework assignment covers unsupervised text classification methods.

## Topic Modeling Breitbart News (50 points)

In this section, we will analyze the thematic structure of a corpus of news articles from Breitbart News, a right-wing American news outlet. Employ a Structural Topic Model from the `stm` library to investigate the themes found within this corpus.

First, we read in a sample of Breitbart articles from 2016 (n=5000):

```{r}
setwd(getwd())
data <- read.csv("breitbart_2016_sample.csv", 
                 stringsAsFactors=FALSE,
                 encoding = "utf-8")
```

1. Process the text and generate a document-feature matrix.  Be sure to remove unhelpful characters and tokens from the DFM and to also retain the original text for model validation.  Remove tokens that occur in less than 20 documents.  Think carefully about your feature selection decisions.

```{r}
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)

# generate corpus object
corpus <-

# create tokens
  
# find collocations
  
# generate and trim dfm
dfm <- 

# parse dates
library(lubridate)
dfm@docvars$date2 <- dmy(dfm@docvars$date)
dfm@docvars$date_month <- floor_date(dfm@docvars$date2, "month")

# attach original text to dfm 
dfm@docvars$original_text <- corpus$documents$content
```

2.  Convert the DFM into STM format and fit an STM model with `k=35` topics.  

```{r}
library(stm)

stmdfm <- convert(dfm, to = "stm") # convert quanteda dfm to stm format (helps with memory)

# fit model
modelFit <- 
```

3.  Examine the topics generated by the STM model and try to interpret some of them using the prevalence and top terms of each topic.    

```{r}
# print top keywords of each topic

# plot prevalence and top terms

```

Pick 5 topics and label them:

a. 

b.

c.

d.

e. 


4.  Topic model validation.  Demonstrate and interpret the semantic and predictive validity of the model.  Also discuss the quality of topics in terms of semantic coherence and top exclusivity.  Discuss how you would show construct validity.

a. Demonstrate predictive validity by plotting the average topic probability over time.

```{r}
library(ggplot2)

# Aggregate topic probability by month
theta_topic <- modelFit$theta[,1]

agg_theta <- aggregate(theta_topic,
                       by = list(month = ??), # Fill in the blank
                       FUN = ??) # Fill in the blank

# Plot aggregated theta over time
ggplot(data = agg_theta,
       aes(x = ??, y = ??, group = 1)) + # Fill in the blanks 
  geom_????() + # Complete the function name
  geom_????() + # Complete the function name
  labs(title = "Topic prevalence",
       x = "Month",
       y = "Average monthly topic probability") + 
  theme_minimal()
```

b. Demonstrate semantic validity via topic correlations

```{r}
# Extract topic correlations
topic_correlations <- 

# Plot correlations
plot.topicCorr(topic_correlations,
               vlabels = seq(1:ncol(modelFit$theta)),
               vertex.color = "white",
               main = "Topic correlations")
```

c. Demonstrate topic quality through semantic coherence

```{r}
SemEx <- as.data.frame(
  cbind(
    c(1:ncol(modelFit$theta)), 
    exclusivity(modelFit),
    semanticCoherence(model = modelFit, 
                      documents = stmdfm$documents,
                      M = 15)
    )
  )

colnames(SemEx) <- c("k", "ex", "coh")

SemExPlot <- ggplot(SemEx, aes(coh, ex)) +
  geom_text(aes(label=k)) +
  labs(x = "Semantic Coherence",
       y = "Exclusivity",
       title = "Topic Semantic Coherence vs. Exclusivity") +
  geom_rug() +
  theme_minimal() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_rect(colour = "gray", size=1))
SemExPlot
```

What insights can be gleaned about right-wing media coverage of the 2016 US election?  What election-related topics were derived from the model?  What interesting temporal patterns exist?  Why might the prevalence of certain important topics vary over 2016?  